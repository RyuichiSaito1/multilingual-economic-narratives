{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAK5AZVCwzVfJlvL1mYpG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/multilingual-economic-narratives/blob/main/notebooks/classification_test_using_gpt_5_nano.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHKEq7uMhHp7",
        "outputId": "33a373e8-8067-4adc-a405-daef4a651b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.12/dist-packages (2.38.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.5.0)\n",
            "Installation completed!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install openai pandas gspread google-auth\n",
        "\n",
        "print(\"Installation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import auth\n",
        "from openai import OpenAI\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk1WYofjheG9",
        "outputId": "15fbf2f0-d349-48ad-ecd2-49f336e9d110"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting up OpenAI API...\")\n",
        "\n",
        "OPENAI_API_KEY = \"key\"\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "print(\"OpenAI client initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZsgEzgEhqcx",
        "outputId": "5c4476f7-eee3-4bc3-f4c3-fee238ec5bd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up OpenAI API...\n",
            "OpenAI client initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing GPT-5 nano model connection...\")\n",
        "\n",
        "try:\n",
        "    test_response = client.chat.completions.create(\n",
        "        model=\"gpt-5-nano\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello, respond with 'OK'\"}],\n",
        "        max_completion_tokens=10\n",
        "    )\n",
        "    print(f\"Model test response: {test_response.choices[0].message.content}\")\n",
        "    print(\"Successfully connected to GPT-5 nano\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to GPT-5 nano: {e}\")\n",
        "    print(\"Note: If GPT-5 nano is not available, you may need to check the model name or your API access\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3br_tf-iBkG",
        "outputId": "e932b8a4-4be2-49a1-a0df-efe531b3d519"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing GPT-5 nano model connection...\n",
            "Model test response: \n",
            "Successfully connected to GPT-5 nano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RedditEconomicSentimentClassifier:\n",
        "    def __init__(self, client):\n",
        "\n",
        "        self.client = client\n",
        "\n",
        "        self.prompt_template = \"\"\"\n",
        "Vous êtes l'économiste en chef du FMI. Je souhaiterais que vous déduisiez les perceptions économiques des citoyens à partir des publications Reddit. Veuillez classer chaque publication Reddit dans l'une des catégories suivantes :\n",
        "\n",
        "0 : La publication traite (1) de l'achat ou de la vente de biens ou de services, (2) des budgets ou des actifs des ménages ou des particuliers, (3) des salaires ou de l'emploi, ou (4) de la conjoncture économique. Dans ce cas, la publication n'exprime pas de sentiment négatif ou indique un sentiment positif.\n",
        "\n",
        "2 : La publication traite (1) de l'achat ou de la vente de biens ou de services, (2) des budgets ou des actifs des ménages ou des particuliers, (3) des salaires ou de l'emploi, ou (4) de la conjoncture économique. Dans ce cas, la publication exprime un sentiment négatif.\n",
        "\n",
        "#1 : La publication n'aborde pas (1) l'achat ou la vente de biens ou de services, (2) les budgets ou les actifs des ménages ou des particuliers, (3) les salaires ou l'emploi, ou (4) la conjoncture économique.\n",
        "\n",
        "# Veuillez choisir une position plus forte lorsque le texte contient à la fois 0 et 2 positions. Si ces positions sont de force égale, répondez 1.\n",
        "\n",
        "Message Reddit à classer : \"{text}\"\n",
        "\n",
        "Répondez uniquement avec le chiffre : 0, 1 ou 2\n",
        "\"\"\"\n",
        "\n",
        "    def create_prompt(self, text):\n",
        "        return self.prompt_template.format(text=text)\n",
        "\n",
        "    def extract_classification(self, response_text):\n",
        "        \"\"\"Extract classification number from model response\"\"\"\n",
        "        # Clean the response\n",
        "        response_text = response_text.strip()\n",
        "\n",
        "        # Look for digits in the response\n",
        "        digits = re.findall(r'\\d', response_text)\n",
        "        if digits:\n",
        "            pred_num = int(digits[0])\n",
        "            if pred_num in [0, 1, 2]:\n",
        "                return pred_num\n",
        "\n",
        "        # Try to find valid number in full response\n",
        "        for valid_class in [0, 1, 2]:\n",
        "            if str(valid_class) in response_text:\n",
        "                return valid_class\n",
        "\n",
        "        # Enhanced pattern matching\n",
        "        patterns = [\n",
        "            r'Classification.*?([012])',\n",
        "            r'Answer.*?([012])',\n",
        "            r'Response.*?([012])',\n",
        "            r'\\b([012])\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, response_text)\n",
        "            if matches:\n",
        "                return int(matches[-1])\n",
        "\n",
        "        return 1  # Default to neutral if unable to extract\n",
        "\n",
        "    def classify_text(self, text, max_retries=3):\n",
        "        \"\"\"Classify a single text using zero-shot prompting\"\"\"\n",
        "        prompt = self.create_prompt(text)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"gpt-5-nano\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert economic sentiment classifier.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.8,\n",
        "                    max_tokens=10\n",
        "                )\n",
        "\n",
        "                response_text = response.choices[0].message.content\n",
        "                classification = self.extract_classification(response_text)\n",
        "                return classification\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in classification attempt {attempt + 1}: {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "        print(\"All attempts failed, defaulting to class 1 (neutral)\")\n",
        "        return 1  # Default to neutral on error\n",
        "\n",
        "    def classify_batch(self, texts, delay=1.0):\n",
        "        \"\"\"Classify a batch of texts with rate limiting\"\"\"\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Processing {i+1}/{len(texts)} texts...\")\n",
        "\n",
        "            result = self.classify_text(text)\n",
        "            results.append(result)\n",
        "            # Delay adjusted for OpenAI API rate limits\n",
        "            time.sleep(delay)\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"RedditEconomicSentimentClassifier class defined successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yncfQ8nbiODs",
        "outputId": "b2d10c51-d2af-4122-9823-4904a23790c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RedditEconomicSentimentClassifier class defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(file_path, start_date=None, end_date=None):\n",
        "    \"\"\"Load TSV data and prepare for processing with optional date filtering\"\"\"\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "    print(f\"Loaded {len(df)} total records\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Convert created_date to datetime\n",
        "    df['created_date'] = pd.to_datetime(df['created_date'])\n",
        "\n",
        "    # Show original date range\n",
        "    print(f\"Original date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Filter by date range if specified\n",
        "    if start_date and end_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: {start_date} <= created_date <= {end_date}\")\n",
        "        df = df[(df['created_date'] >= start_date) & (df['created_date'] <= end_date)]\n",
        "        print(f\"Filtered to date range {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif start_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        print(f\"Applying filter: created_date >= {start_date}\")\n",
        "        df = df[df['created_date'] >= start_date]\n",
        "        print(f\"Filtered from {start_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif end_date:\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: created_date <= {end_date}\")\n",
        "        df = df[df['created_date'] <= end_date]\n",
        "        print(f\"Filtered to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "\n",
        "    # Add year_month column for grouping\n",
        "    df['year_month'] = df['created_date'].dt.to_period('M')\n",
        "\n",
        "    print(f\"Final date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Debug: Show the unique year_month values to verify filtering\n",
        "    unique_months = sorted(df['year_month'].unique())\n",
        "    print(f\"Unique year_month values after filtering: {unique_months}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def sample_monthly_data(df, max_samples_per_month=800):\n",
        "    \"\"\"Sample up to max_samples_per_month records per month\"\"\"\n",
        "    sampled_dfs = []\n",
        "\n",
        "    for year_month, group in df.groupby('year_month'):\n",
        "        if len(group) <= max_samples_per_month:\n",
        "            sampled_group = group\n",
        "        else:\n",
        "            sampled_group = group.sample(n=max_samples_per_month, random_state=42)\n",
        "\n",
        "        sampled_dfs.append(sampled_group)\n",
        "        print(f\"{year_month}: sampled {len(sampled_group)} out of {len(group)} records\")\n",
        "\n",
        "    result_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "    print(f\"Total sampled records: {len(result_df)}\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def process_and_classify_data(df, classifier, output_path, spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Process data month by month and classify texts\"\"\"\n",
        "    # Sort by date\n",
        "    df = df.sort_values('created_date')\n",
        "\n",
        "    # Initialize Google Sheets connection\n",
        "    try:\n",
        "        # Use Google Colab authentication\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Get default credentials\n",
        "        from google.auth import default\n",
        "        creds, _ = default()\n",
        "\n",
        "        # Create gspread client\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "        # Open spreadsheet\n",
        "        spreadsheet = gc.open(spreadsheet_name)\n",
        "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
        "\n",
        "        # Check if worksheet has data and add headers if empty\n",
        "        existing_data = worksheet.get_all_values()\n",
        "        if not existing_data:\n",
        "            headers = ['year_month', 'positive', 'neutral', 'negative', 'total_number', 'average_score']\n",
        "            worksheet.append_row(headers)\n",
        "\n",
        "        print(f\"Connected to existing spreadsheet: {spreadsheet_name} - {worksheet_name}\")\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"Error: Spreadsheet '{spreadsheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Error: Worksheet '{worksheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to Google Sheet: {type(e).__name__}: {str(e)}\")\n",
        "        worksheet = None\n",
        "\n",
        "    # Group by year_month for processing\n",
        "    for year_month, month_group in df.groupby('year_month'):\n",
        "        print(f\"\\nProcessing {year_month}...\")\n",
        "\n",
        "        # Create a copy to avoid modifying original\n",
        "        month_df = month_group.copy()\n",
        "\n",
        "        # Classify texts\n",
        "        texts = month_df['body'].fillna('').astype(str).tolist()\n",
        "        classifications = classifier.classify_batch(texts)\n",
        "\n",
        "        # Add classifications to dataframe\n",
        "        month_df['sentiment'] = classifications\n",
        "\n",
        "        # Remove year_month column before saving to TSV\n",
        "        month_df_tsv = month_df.drop('year_month', axis=1)\n",
        "\n",
        "        # Save to TSV file (append mode)\n",
        "        if os.path.exists(output_path):\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='a', header=False, index=False)\n",
        "        else:\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='w', header=True, index=False)\n",
        "\n",
        "        # Calculate monthly summary\n",
        "        positive_count = len(month_df[month_df['sentiment'] == 0])\n",
        "        neutral_count = len(month_df[month_df['sentiment'] == 1])\n",
        "        negative_count = len(month_df[month_df['sentiment'] == 2])\n",
        "        total_count = len(month_df)\n",
        "        avg_score = (positive_count * 0 + neutral_count * 1 + negative_count * 2) / total_count\n",
        "\n",
        "        # Output to standard output\n",
        "        print(f\"Month: {year_month}\")\n",
        "        print(f\"  Positive (0): {positive_count}\")\n",
        "        print(f\"  Neutral (1): {neutral_count}\")\n",
        "        print(f\"  Negative (2): {negative_count}\")\n",
        "        print(f\"  Total: {total_count}\")\n",
        "        print(f\"  Average Score: {avg_score:.4f}\")\n",
        "\n",
        "        # Add to Google Sheet\n",
        "        if worksheet:\n",
        "            try:\n",
        "                row_data = [str(year_month), positive_count, neutral_count, negative_count, total_count, round(avg_score, 4)]\n",
        "                worksheet.append_row(row_data)\n",
        "                print(f\"  Added to Google Sheet: {spreadsheet_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error updating Google Sheet: {e}\")\n",
        "\n",
        "        print(f\"Saved {len(month_df)} records for {year_month}\")\n",
        "\n",
        "    print(f\"\\nAll data processed and saved to: {output_path}\")\n",
        "\n",
        "print(\"Helper functions defined successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKaZU2yzmyZx",
        "outputId": "af69da0a-8b1b-4f36-de3d-b74b148ba668"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "input_file = \"/content/drive/MyDrive/multilingual-economic-narratives/data/test/france_comments.tsv\"\n",
        "output_file = \"/content/drive/MyDrive/multilingual-economic-narratives/data/test/france_comments_results.tsv\"\n",
        "\n",
        "# Google Sheets configuration\n",
        "spreadsheet_name = \"Classification_Test\"\n",
        "worksheet_name = \"france_20251010\"\n",
        "\n",
        "# Date range configuration\n",
        "start_date = \"2020-03-01\"\n",
        "end_date = \"2020-04-30\"\n",
        "\n",
        "# Maximum samples per month\n",
        "max_samples_per_month = 400\n",
        "\n",
        "print(\"Configuration completed\")\n",
        "print(f\"Input file: {input_file}\")\n",
        "print(f\"Output file: {output_file}\")\n",
        "print(f\"Spreadsheet: {spreadsheet_name} - {worksheet_name}\")\n",
        "print(f\"Date range: {start_date} to {end_date}\")\n",
        "print(f\"Max samples per month: {max_samples_per_month}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GduiXhSor2v",
        "outputId": "318440ee-d7c7-4d92-e4a5-c3c6bfd1366f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration completed\n",
            "Input file: /content/drive/MyDrive/multilingual-economic-narratives/data/test/france_comments.tsv\n",
            "Output file: /content/drive/MyDrive/multilingual-economic-narratives/data/test/france_comments_results.tsv\n",
            "Spreadsheet: Classification_Test - france_20251010\n",
            "Date range: 2020-03-01 to 2020-04-30\n",
            "Max samples per month: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "print(\"Output directory created/verified\")\n",
        "\n",
        "classifier = RedditEconomicSentimentClassifier(client)\n",
        "print(\"Classifier initialized successfully\")\n",
        "\n",
        "df = load_and_prepare_data(input_file, start_date, end_date)\n",
        "print(f\"\\nData loaded successfully Shape: {df.shape}\")\n",
        "\n",
        "sampled_df = sample_monthly_data(df, max_samples_per_month=max_samples_per_month)\n",
        "print(f\"\\nData sampling completed Final shape: {sampled_df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09lDElZQsO9V",
        "outputId": "13aa3a25-d819-412a-8332-a136713fb20f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory created/verified\n",
            "Classifier initialized successfully\n",
            "Loading data from: /content/drive/MyDrive/multilingual-economic-narratives/data/test/france_comments.tsv\n",
            "Loaded 1447845 total records\n",
            "Columns: ['created_date', 'subreddit_id', 'id', 'author', 'parent_id', 'body', 'score']\n",
            "Original date range: 2016-01-01 00:52:55 to 2022-12-31 23:59:09\n",
            "Applying filter: 2020-03-01 00:00:00 <= created_date <= 2020-04-30 00:00:00\n",
            "Filtered to date range 2020-03-01 to 2020-04-30: 44275 records\n",
            "Final date range: 2020-03-01 00:05:40 to 2020-04-29 23:58:01\n",
            "Unique year_month values after filtering: [Period('2020-03', 'M'), Period('2020-04', 'M')]\n",
            "\n",
            "Data loaded successfully Shape: (44275, 8)\n",
            "2020-03: sampled 400 out of 22554 records\n",
            "2020-04: sampled 400 out of 21721 records\n",
            "Total sampled records: 800\n",
            "\n",
            "Data sampling completed Final shape: (800, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING CLASSIFICATION PROCESS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Using Google Spreadsheet: {spreadsheet_name}\")\n",
        "print(f\"Using Worksheet: {worksheet_name}\")\n",
        "print(f\"Date range: {start_date} to {end_date}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Process and classify data with real-time Google Sheets updates\n",
        "process_and_classify_data(sampled_df, classifier, output_file, spreadsheet_name, worksheet_name)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ZERO-SHOT PROCESS COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Monthly summaries updated in Google Sheets: {spreadsheet_name} - {worksheet_name}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "V3sNjghls5EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAC21ZFiVObT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}