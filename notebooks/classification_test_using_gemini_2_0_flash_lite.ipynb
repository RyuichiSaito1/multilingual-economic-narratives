{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd+eKytqxF774Ob2ZFG+v1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuichiSaito1/multilingual-economic-narratives/blob/main/notebooks/classification_test_using_gemini_2_0_flash_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gk3IkxwgwFf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install google-cloud-aiplatform pandas gspread google-auth\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "import gspread\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "MUuNh3ZWhSle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Cloud Authentication Setup\n",
        "print(\"Authenticating with Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authentication completed!\")\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = \"###\"\n",
        "LOCATION = \"###\"\n",
        "\n",
        "print(\"Initializing Vertex AI...\")\n",
        "try:\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "    print(\"Vertex AI initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Vertex AI: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "TLNw9ZgEha2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini model\n",
        "print(\"Initializing Gemini model...\")\n",
        "try:\n",
        "    model = GenerativeModel(\"gemini-2.0-flash-lite\")\n",
        "    print(\"Model initialized successfully!\")\n",
        "\n",
        "    # Test the model connection\n",
        "    print(\"Testing model connection...\")\n",
        "    test_response = model.generate_content(\"Hello, respond with 'OK'\")\n",
        "    print(f\"Model test response: {test_response.text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing model: {e}\")\n",
        "    # Try alternative model names\n",
        "    alternative_models = [\n",
        "        \"gemini-2.0-flash-exp\",\n",
        "        \"gemini-1.5-flash\",\n",
        "        \"gemini-1.5-pro\"\n",
        "    ]\n",
        "\n",
        "    model = None\n",
        "    for model_name in alternative_models:\n",
        "        try:\n",
        "            print(f\"Trying {model_name}...\")\n",
        "            model = GenerativeModel(model_name)\n",
        "            test_response = model.generate_content(\"Hello, respond with 'OK'\")\n",
        "            print(f\"Successfully connected to {model_name}\")\n",
        "            break\n",
        "        except Exception as model_error:\n",
        "            continue\n",
        "\n",
        "    if model is None:\n",
        "        raise Exception(\"No Gemini model available\")\n",
        "\n",
        "print(f\"Successfully connected to model\")\n"
      ],
      "metadata": {
        "id": "7m0iUOBniBUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RedditInflationClassifier:\n",
        "    def __init__(self, model):\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        self.prompt_template = \"\"\"Είστε επικεφαλής οικονομολόγος στο ΔΝΤ. Θα ήθελα να συμπεράνετε την αντίληψη του κοινού για τον πληθωρισμό από αναρτήσεις στο Reddit. Παρακαλώ ταξινομήστε κάθε ανάρτηση στο Reddit σε μία από τις ακόλουθες κατηγορίες:\n",
        "\n",
        "0: Η ανάρτηση υποδεικνύει αποπληθωρισμό, όπως τη χαμηλότερη τιμή αγαθών ή υπηρεσιών (π.χ., \"οι τιμές δεν είναι κακές\"), προσιτές υπηρεσίες (π.χ., \"αυτή η σαμπάνια είναι φθηνή και νόστιμη\"), πληροφορίες πωλήσεων (π.χ., \"μπορείτε να την αποκτήσετε μόνο με 10 δολάρια.\") ή μια φθίνουσα αγορά αγοραστή.\n",
        "\n",
        "2: Η ανάρτηση υποδεικνύει ή περιλαμβάνει πληθωρισμό, όπως την υψηλότερη τιμή αγαθών ή υπηρεσιών (π.χ., \"δεν είναι φθηνή\"), το παράλογο κόστος αγαθών ή υπηρεσιών (π.χ., \"το φαγητό είναι υπερτιμημένο και κρύο\"), καταναλωτές που αγωνίζονται να αγοράσουν τα απαραίτητα (π.χ., \"τα είδη είναι πολύ ακριβά για να αγοραστούν\"), έλλειψη αγαθών ή υπηρεσιών ή αναφορά σε μια φούσκα περιουσιακών στοιχείων.\n",
        "\n",
        "1: Η ανάρτηση δεν υποδεικνύει ούτε αποπληθωρισμό (0) ούτε πληθωρισμό (2). Αυτή η κατηγορία περιλαμβάνει επίσης μόνο ερωτήσεις προς μια κοινότητα, κοινωνικές δηλώσεις, όχι προσωπικές. εμπειρία, πραγματικές παρατηρήσεις, αναφορές σε αρχικά ακριβά ή φθηνά αγαθά ή υπηρεσίες (π.χ., \"ένα πανέμορφο και ακριβό δείπνο\" ή \"ένα προσιτό Civic\"), προώθηση ιστοσελίδας, επιθυμίες συγγραφέων ή παράλογο κείμενο.\n",
        "\n",
        "Επιλέξτε μια πιο ισχυρή στάση όταν το κείμενο περιλαμβάνει τόσο 0 όσο και 2 θέσεις. Εάν αυτές οι θέσεις είναι του ίδιου βαθμού, απαντήστε 1.\n",
        "\n",
        "Δημοσίευση Reddit για ταξινόμηση: \"{text}\"\n",
        "\n",
        "Απαντήστε μόνο με τον αριθμό: 0, 1 ή 2\"\"\"\n",
        "\n",
        "    def create_prompt(self, text):\n",
        "        return self.prompt_template.format(text=text)\n",
        "\n",
        "    def extract_classification(self, response_text):\n",
        "        \"\"\"Extract classification number from model response - ENHANCED ERROR HANDLING\"\"\"\n",
        "        # Clean the response\n",
        "        response_text = response_text.strip()\n",
        "\n",
        "        # Look for digits in the response\n",
        "        digits = re.findall(r'\\d', response_text)\n",
        "        if digits:\n",
        "            pred_num = int(digits[0])\n",
        "            if pred_num in [0, 1, 2]:\n",
        "                return pred_num\n",
        "\n",
        "        # Try to find valid number in full response\n",
        "        for valid_class in [0, 1, 2]:\n",
        "            if str(valid_class) in response_text:\n",
        "                return valid_class\n",
        "\n",
        "        # Enhanced pattern matching\n",
        "        patterns = [\n",
        "            r'Classification.*?([012])',\n",
        "            r'Answer.*?([012])',\n",
        "            r'Response.*?([012])',\n",
        "            r'\\b([012])\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, response_text)\n",
        "            if matches:\n",
        "                return int(matches[-1])\n",
        "\n",
        "        return 1  # Default to neutral if unable to extract\n",
        "\n",
        "    def classify_text(self, text, max_retries=3):\n",
        "        \"\"\"Classify a single text using zero-shot prompting\"\"\"\n",
        "        prompt = self.create_prompt(text)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # CHANGE: Use Gemini model with generation config instead of fine-tuned model\n",
        "                generation_config = {\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"top_p\": 0.8,\n",
        "                    \"top_k\": 40,\n",
        "                    \"max_output_tokens\": 10,\n",
        "                }\n",
        "\n",
        "                response = self.model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config=generation_config\n",
        "                )\n",
        "\n",
        "                classification = self.extract_classification(response.text)\n",
        "                return classification\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in classification attempt {attempt + 1}: {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "        print(\"All attempts failed, defaulting to class 1 (neutral)\")\n",
        "        return 1  # Default to neutral on error\n",
        "\n",
        "    def classify_batch(self, texts, delay=1.0):\n",
        "        \"\"\"Classify a batch of texts with rate limiting - INCREASED DELAY FOR API LIMITS\"\"\"\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Processing {i+1}/{len(texts)} texts...\")\n",
        "\n",
        "            result = self.classify_text(text)\n",
        "            results.append(result)\n",
        "            # CHANGE: Increased delay to handle API rate limits for base model\n",
        "            time.sleep(delay)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "fXdooCXeiXTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(file_path, start_date=None, end_date=None):\n",
        "    \"\"\"Load TSV data and prepare for processing with optional date filtering\"\"\"\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "    # Read TSV file\n",
        "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "    print(f\"Loaded {len(df)} total records\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Convert created_date to datetime\n",
        "    df['created_date'] = pd.to_datetime(df['created_date'])\n",
        "\n",
        "    # Show original date range\n",
        "    print(f\"Original date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Debug: Show some sample dates before filtering\n",
        "    print(f\"Sample dates from original data:\")\n",
        "    sample_dates = df['created_date'].head(10).tolist()\n",
        "    for i, date in enumerate(sample_dates):\n",
        "        print(f\"  {i+1}: {date}\")\n",
        "\n",
        "    # Filter by date range if specified\n",
        "    if start_date and end_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: {start_date} <= created_date <= {end_date}\")\n",
        "        df = df[(df['created_date'] >= start_date) & (df['created_date'] <= end_date)]\n",
        "        print(f\"Filtered to date range {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif start_date:\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        print(f\"Applying filter: created_date >= {start_date}\")\n",
        "        df = df[df['created_date'] >= start_date]\n",
        "        print(f\"Filtered from {start_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "    elif end_date:\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        print(f\"Applying filter: created_date <= {end_date}\")\n",
        "        df = df[df['created_date'] <= end_date]\n",
        "        print(f\"Filtered to {end_date.strftime('%Y-%m-%d')}: {len(df)} records\")\n",
        "\n",
        "    # Add year_month column for grouping\n",
        "    df['year_month'] = df['created_date'].dt.to_period('M')\n",
        "\n",
        "    print(f\"Final date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
        "\n",
        "    # Debug: Show the unique year_month values to verify filtering\n",
        "    unique_months = sorted(df['year_month'].unique())\n",
        "    print(f\"Unique year_month values after filtering: {unique_months}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def sample_monthly_data(df, max_samples_per_month=800):\n",
        "  \"\"\"Sample up to max_samples_per_month records per month\"\"\"\n",
        "  sampled_dfs = []\n",
        "\n",
        "  for year_month, group in df.groupby('year_month'):\n",
        "      if len(group) <= max_samples_per_month:\n",
        "          sampled_group = group\n",
        "      else:\n",
        "          sampled_group = group.sample(n=max_samples_per_month, random_state=42)\n",
        "\n",
        "      sampled_dfs.append(sampled_group)\n",
        "      print(f\"{year_month}: sampled {len(sampled_group)} out of {len(group)} records\")\n",
        "\n",
        "  result_df = pd.concat(sampled_dfs, ignore_index=True)\n",
        "  print(f\"Total sampled records: {len(result_df)}\")\n",
        "\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "Lg3UHhFY6HhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_classify_data(df, classifier, output_path, spreadsheet_name, worksheet_name):\n",
        "    \"\"\"Process data month by month and classify texts\"\"\"\n",
        "    # Sort by date\n",
        "    df = df.sort_values('created_date')\n",
        "\n",
        "    # Initialize Google Sheets connection\n",
        "    try:\n",
        "        # Use Google Colab authentication\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Get default credentials\n",
        "        from google.auth import default\n",
        "        creds, _ = default()\n",
        "\n",
        "        # Create gspread client\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "        # Open spreadsheet\n",
        "        spreadsheet = gc.open(spreadsheet_name)\n",
        "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
        "\n",
        "        # Check if worksheet has data and add headers if empty\n",
        "        existing_data = worksheet.get_all_values()\n",
        "        if not existing_data:\n",
        "            headers = ['year_month', 'deflation', 'neither', 'inflation', 'total_number', 'average_score']\n",
        "            worksheet.append_row(headers)\n",
        "\n",
        "        print(f\"Connected to existing spreadsheet: {spreadsheet_name} - {worksheet_name}\")\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"Error: Spreadsheet '{spreadsheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Error: Worksheet '{worksheet_name}' not found. Please check the name.\")\n",
        "        worksheet = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to Google Sheet: {type(e).__name__}: {str(e)}\")\n",
        "        worksheet = None\n",
        "\n",
        "    # Group by year_month for processing\n",
        "    for year_month, month_group in df.groupby('year_month'):\n",
        "        print(f\"\\nProcessing {year_month}...\")\n",
        "\n",
        "        # Create a copy to avoid modifying original\n",
        "        month_df = month_group.copy()\n",
        "\n",
        "        # Classify texts\n",
        "        texts = month_df['body'].fillna('').astype(str).tolist()\n",
        "        classifications = classifier.classify_batch(texts)\n",
        "\n",
        "        # Add classifications to dataframe\n",
        "        month_df['inflation'] = classifications\n",
        "\n",
        "        # Remove year_month column before saving to TSV\n",
        "        month_df_tsv = month_df.drop('year_month', axis=1)\n",
        "\n",
        "        # Save to TSV file (append mode)\n",
        "        if os.path.exists(output_path):\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='a', header=False, index=False)\n",
        "        else:\n",
        "            month_df_tsv.to_csv(output_path, sep='\\t', mode='w', header=True, index=False)\n",
        "\n",
        "        # Calculate monthly summary\n",
        "        deflation_count = len(month_df[month_df['inflation'] == 0])\n",
        "        neither_count = len(month_df[month_df['inflation'] == 1])\n",
        "        inflation_count = len(month_df[month_df['inflation'] == 2])\n",
        "        total_count = len(month_df)\n",
        "        avg_score = (deflation_count * 0 + neither_count * 1 + inflation_count * 2) / total_count\n",
        "\n",
        "        # Output to standard output\n",
        "        print(f\"Month: {year_month}\")\n",
        "        print(f\"  Deflation (0): {deflation_count}\")\n",
        "        print(f\"  Neither (1): {neither_count}\")\n",
        "        print(f\"  Inflation (2): {inflation_count}\")\n",
        "        print(f\"  Total: {total_count}\")\n",
        "        print(f\"  Average Score: {avg_score:.4f}\")\n",
        "\n",
        "        # Add to Google Sheet\n",
        "        if worksheet:\n",
        "            try:\n",
        "                row_data = [str(year_month), deflation_count, neither_count, inflation_count, total_count, round(avg_score, 4)]\n",
        "                worksheet.append_row(row_data)\n",
        "                print(f\"  Added to Google Sheet: {spreadsheet_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error updating Google Sheet: {e}\")\n",
        "\n",
        "        print(f\"Saved {len(month_df)} records for {year_month}\")\n",
        "\n",
        "    print(f\"\\nAll data processed and saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "fQMBI14t61JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # File paths\n",
        "    input_file = \"/content/drive/MyDrive/multilingual-economic-narratives/data/test/greece_comments.tsv\"\n",
        "    output_file = \"/content/drive/MyDrive/multilingual-economic-narratives/data/test/greece_comments_results.tsv\"\n",
        "\n",
        "    # Google Sheets configuration\n",
        "    spreadsheet_name = \"Classification_Test\"\n",
        "    worksheet_name = \"greece\"\n",
        "\n",
        "    # Date range configuration\n",
        "    start_date = \"2016-01-01\"\n",
        "    end_date = \"2022-12-31\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "    # Initialize classifier with model instance instead of endpoint and client\n",
        "    classifier = RedditInflationClassifier(model)\n",
        "\n",
        "    # Load and prepare data with date filtering\n",
        "    df = load_and_prepare_data(input_file, start_date, end_date)\n",
        "\n",
        "    # Sample monthly data\n",
        "    sampled_df = sample_monthly_data(df, max_samples_per_month=300)\n",
        "\n",
        "    print(f\"Using Google Spreadsheet: {spreadsheet_name}\")\n",
        "    print(f\"Using Worksheet: {worksheet_name}\")\n",
        "    if start_date:\n",
        "        print(f\"Date range: {start_date} to {end_date if end_date else 'latest'}\")\n",
        "\n",
        "    # Process and classify data with real-time Google Sheets updates\n",
        "    process_and_classify_data(sampled_df, classifier, output_file, spreadsheet_name, worksheet_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ZERO-SHOT PROCESS COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "    print(f\"Monthly summaries updated in Google Sheets: {spreadsheet_name} - {worksheet_name}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lNmRHAIf7LLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfcZNWRFBBkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}